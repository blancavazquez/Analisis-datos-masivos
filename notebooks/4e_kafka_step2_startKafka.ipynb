{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "793c2149-b771-4722-b1e5-05e7c3169cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parte 2: Inicio de Apache Kafka\n",
    "\n",
    "El objetivo de esta libreta es Ãºnicamente dar inicio al servicio de Apache Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86b3976-ff65-4180-a3cb-cbf9bdad22d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 119384\n-rw-r--r-- 1 root root 120900366 Jul 26 17:06 kafka_2.12-3.8.0.tgz\ndrwxr-xr-x 2 root root      4096 Oct  8 16:30 conf\n-r-xr-xr-x 1 root root      2755 Oct  8 16:30 hadoop_accessed_config.lst\ndrwxr-xr-x 2 root root      4096 Oct  8 16:30 azure\n-r-xr-xr-x 1 root root   1306936 Oct  8 16:30 preload_class.lst\ndrwxr-xr-x 3 root root      4096 Oct  8 20:41 eventlogs\ndrwxr-xr-x 5 root root      4096 Oct  8 20:45 metastore_db\ndrwxr-xr-x 2 root root      4096 Oct  8 21:01 logs\n-rw-r--r-- 1 root root      2254 Oct  8 21:01 craft-popular-urls\ndrwxr-xr-x 8 root root      4096 Oct  8 21:52 kafka_2.12-3.8.0\ndrwxr-xr-x 2 root root      4096 Oct  8 21:55 ganglia\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.logging.log4j--log4j-slf4j-impl--org.apache.logging.log4j__log4j-slf4j-impl__2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/databricks/driver/kafka_2.12-3.8.0/libs/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[2024-10-08 21:57:04,294] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2024-10-08 21:57:05,664] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2024-10-08 21:57:05,672] INFO RemoteLogManagerConfig values: \n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-08 21:57:06,273] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2024-10-08 21:57:06,286] INFO starting (kafka.server.KafkaServer)\n[2024-10-08 21:57:06,288] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n[2024-10-08 21:57:06,417] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n[2024-10-08 21:57:06,439] INFO Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT (org.apache.zookeeper.ZooKeeper)\n[2024-10-08 21:57:06,439] INFO Client environment:host.name=1008-204032-lfazxauf-10-172-210-210 (org.apache.zookeeper.ZooKeeper)\n[2024-10-08 21:57:06,439] INFO Client environment:java.version=1.8.0_382 (org.apache.zookeeper.ZooKeeper)\n[2024-10-08 21:57:06,439] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)\n[2024-10-08 21:57:06,439] INFO Client environment:java.home=/usr/lib/jvm/zulu8-ca-amd64/jre (org.apache.zookeeper.ZooKeeper)\n[2024-10-08 21:57:06,440] INFO Client environment:java.class.path=/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-shims--org.apache.hive__hive-shims__2.3.9.jar:/databricks/jars/----ws_3_3--vendor--protobuf--protobuf-hive-2.3__hadoop-3.2_2.12_shaded---1958151750--protobuf-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--jackson--shaded_dependencies--1999524332--com.google.code.findbugs__jsr305__2.0.1.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--945576458--io.perfmark__perfmark-api__0.23.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12--1258094998--javax.inject__javax.inject__1.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded--1770584804--com.lmax__disruptor__3.4.2.jar:/databricks/jars/proto--linter--proto--linter_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1047197947--io.projectreactor__reactor-core__3.4.8.jar:/databricks/jars/----ws_3_3--vendor--avro--avro-hive-2.3__hadoop-3.2_2.12_shaded---1954496799--avro-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1570573510--io.netty__netty-tcnative-boringssl-static-linux-x86_64__2.0.52.Final.jar:/databricks/jars/common--encryption--cpk-deps-shaded--2084285423--io.netty__netty-resolver-dns__4.1.65.Final.jar:/databricks/jars/----ws_3_3--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/common--threading--idle-indicator--idle-indicator-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--19273035--com.google.flogger__flogger__0.5.1.jar:/databricks/jars/third_party--jsonwebtoken--jsonwebtoken_shaded--357284269--com.fasterxml.jackson.core__jackson-core__2.9.10.jar:/databricks/jars/spark--driver--events-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1258740422--com.google.apis__google-api-services-serviceusage__v1-rev20201208-1.31.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/common--destroyable--destroyable-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---330209066--io.netty__netty-codec__4.1.77.Final.jar:/databricks/jars/common--error-code--error-code-utils-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1447657062--com.google.auth__google-auth-library-credentials__0.22.2.jar:/databricks/jars/libraries--libraries-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---902205190--org.checkerframework__checker-compat-qual__2.5.5.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1865046184--com.azure__azure-security-keyvault-secrets__4.3.2.jar:/databricks/jars/common--database--utils--username-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1862825633--com.google.http-client__google-http-client-jackson2__1.38.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1043467615--io.opencensus__opencensus-contrib-http-util__0.28.0.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1574959432--io.netty__netty-codec__4.1.74.Final.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---1367335563--com.google.guava__failureaccess__1.0.1.jar:/databricks/jars/common--encryption--cpk-deps-shaded---1211039700--io.netty__netty-codec-http__4.1.87.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.razorvine--pickle--net.razorvine__pickle__1.2.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded--1605216124--com.squareup.okio__okio__1.13.0.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/third_party--armeria--armeria_shaded---765956118--com.aayushatharva.brotli4j__brotli4j__1.7.1.jar:/databricks/jars/third_party--hadoop--hadoop-tools--hadoop-aws--hadoop-aws-shaded-spark_3.3_2.12--440711912--lib-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--327920997--software.amazon.awssdk__kinesis__2.17.190.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---221577655--com.google.apis__google-api-services-compute__v1-rev20220526-1.32.1.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1787300611--com.fasterxml.woodstox__woodstox-core__6.2.7.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.36.jar:/databricks/jars/third_party--zeromq--zeromq_shaded---1498094674--org.zeromq__jeromq__0.5.2.jar:/databricks/jars/common--logging--metrics--query-insights--query-insights-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----ws_3_3--vendor--redshift--redshift-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/api-base--api-base_java-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--68992652--io.netty__netty-resolver-dns-native-macos-osx-x86_64__4.1.77.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/common--database--datasource--utils-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.netty--netty-codec--io.netty__netty-codec__4.1.74.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---317441679--com.google.apis__google-api-services-storage__v1-rev20201112-1.31.0.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1129572957--software.amazon.awssdk__cloudwatch__2.15.31.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1411025232--com.typesafe.netty__netty-reactive-streams-http__2.0.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--oro--oro--oro__oro__2.0.8.jar:/databricks/jars/----ws_3_3--safespark--udf--common--client-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--ip-address--ip-address-spark_3.3_2.12_deploy.jar:/databricks/jars/common--chauffeur--testutil--testutil-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--activity--sqlgateway--sqlgateway_service_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.orc--orc-core--org.apache.orc__orc-core__1.7.8.jar:/databricks/jars/common--logging--tags--encryption--encryption-spark_3.3_2.12_deploy.jar:/databricks/jars/jsonutil--jsonutil-spark_3.3_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1877322253--com.azure__azure-core-http-netty__1.10.2.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----ws_3_3--vendor--sql-gcp-connectors--pubsub_scala_proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--1283626578--software.amazon.glue__schema-registry-serde__1.0.0.jar:/databricks/jars/common--database--trace-sql-log--trace-sql-log-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.eclipse.jetty.websocket--websocket-client--org.eclipse.jetty.websocket__websocket-client__9.4.46.v20220331.jar:/databricks/jars/----ws_3_3--sql--hive--libhive_resources.jar:/databricks/jars/third_party--armeria--armeria_shaded--1989066408--org.reactivestreams__reactive-streams__1.0.3.jar:/databricks/jars/common--logging--lineage--lineage-spark_3.3_2.12_deploy.jar:/databricks/jars/common--spark--conf-option-spec--conf-option-spec-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk--671268491--software.amazon.awssdk__auth__2.17.190.jar:/databricks/jars/common--activity-context--activity-context-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.typelevel--cats-kernel_2.12--org.typelevel__cats-kernel_2.12__2.1.1.jar:/databricks/jars/third_party--armeria--armeria_shaded--458243678--com.google.protobuf__protobuf-java__3.19.2.jar:/databricks/jars/common--logging--structured--proto-logger-spark_3.3_2.12_deploy.jar:/databricks/jars/common--logging--metrics--metrics-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---2128655557--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1724254641--com.google.http-client__google-http-client__1.39.0.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--785318626--com.google.auth__google-auth-library-oauth2-http__0.22.2.jar:/databricks/jars/common--jetty--server--server_deprecated-spark_3.3_2.12_deploy.jar:/databricks/jars/compliance--taxonomy--taxonomy_cross_scala_lib-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---645991120--io.netty__netty-tcnative-boringssl-static-linux-x86_64__2.0.52.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/common--pricing--pricing-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--integration-test-log--integration_test_log_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--sql--catalyst--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--705909857--com.google.http-client__google-http-client-jackson2__1.39.0.jar:/databricks/jars/proto--logs--pipelines--pipelines_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/api-base--proto--exception_with_details_proto_scala-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--msal4j--msal4j-shaded--126169597--com.nimbusds__content-type__2.3.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---412092703--io.projectreactor__reactor-core__3.4.19.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/common--encryption--proto--encryption_proto-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--activity--config-manager--config_manager_service_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--util--validation-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-cli--commons-cli--commons-cli__commons-cli__1.5.0.jar:/databricks/jars/----ws_3_3--vendor--sql-azure-connectors--libsql-azure-connectors_resources.jar:/databricks/jars/third_party--armeria--armeria_shaded---2067219489--com.fasterxml.jackson.core__jackson-databind__2.13.5.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/spark--common--spark-common-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---84348661--com.google.api.grpc__proto-google-common-protos__2.1.0.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--442951015--com.google.apis__google-api-services-iam__v1-rev316-1.25.0.jar:/databricks/jars/common--languages--languages-spark_3.3_2.12_deploy.jar:/databricks/jars/common--util--execution-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1203611685--com.google.flogger__google-extensions__0.5.1.jar:/databricks/jars/common--logging--redactor--usage_log_redactor-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded--1566512993--com.google.cloud__google-cloud-core__1.94.3.jar:/databricks/jars/common--conf--common--databricks-main-conf-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.jersey.inject--jersey-hk2--org.glassfish.jersey.inject__jersey-hk2__2.36.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---439083003--io.opencensus__opencensus-contrib-http-util__0.28.0.jar:/databricks/jars/----ws_3_3--core--libcore_resources.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---2084309403--com.azure__azure-security-keyvault-keys__4.4.4.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----ws_3_3--third_party--kinesis-sdk--kinesis-sdk---722831861--software.amazon.awssdk__dynamodb__2.15.31.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--557535904--io.netty__netty-codec-xml__4.1.77.Final.jar:/databricks/jars/----ws_3_3--vendor--protobuf--kafka-clients_only_shaded-for-protobuf-hive-2.3__hadoop-3.2---614483296--org.apache.kafka__kafka-clients__7.4.0-ccs.jar:/databricks/jars/common--logging--service-request--logger-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--dev.ludovic.netlib--blas--dev.ludovic.netlib__blas__2.2.1.jar:/databricks/jars/common--rpc--parser--parser-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--819614115--io.netty__netty-transport-native-kqueue-osx-x86_64__4.1.74.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1612111443--com.google.http-client__google-http-client__1.38.0.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.3_2.12_deploy.jar:/databricks/jars/common--database--config--config-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--103420347--io.netty__netty-codec-mqtt__4.1.77.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1579378425--io.netty__netty-common__4.1.74.Final.jar:/databricks/jars/common--rpc--metrics--server-metrics-recorder-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.9.0.jar:/databricks/jars/common--error-code--error-code-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded--1928460320--io.netty__netty-codec__4.1.77.Final.jar:/databricks/jars/cluster-common--storage-context-type-spark_3.3_2.12_deploy.jar:/databricks/jars/common--user--user-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/spark--driver--safespark-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.3.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded---845473700--io.opentracing__opentracing-api__0.31.0.jar:/databricks/jars/common--logging--activity--logger--logger-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---1874608063--io.netty__netty-tcnative-boringssl-static-osx-x86_64__2.0.53.Final.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.hive--hive-serde--org.apache.hive__hive-serde__2.3.9.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.3_2.12---574220505--io.reactivex__rxjava__1.2.4.jar:/databricks/jars/rcp--common-api--common-api-spark_3.3_2.12_deploy.jar:/databricks/jars/proto--logs--activity--request_activity_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1122468363--io.netty__netty-codec-socks__4.1.87.Final.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---346995935--io.netty__netty-common__4.1.77.Final.jar:/databricks/jars/proto--logs--observability--log_daemon_event_log_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--io.netty--netty-common--io.netty__netty-common__4.1.74.Final.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---235794917--io.grpc__grpc-protobuf__1.37.0.jar:/databricks/jars/third_party--armeria--armeria_shaded---85807505--io.netty__netty-tcnative-boringssl-static-osx-aarch_64__2.0.52.Final.jar:/databricks/jars/common--logging--activity--builder--builder-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.ivy--ivy--org.apache.ivy__ivy__2.5.1.jar:/databricks/jars/third_party--armeria--armeria_shaded--2056364852--com.linecorp.armeria__armeria-grpc-protocol__1.16.0.jar:/databricks/jars/common--command-result--command-result-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--sql--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--http--headers--headers-spark_3.3_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded---460269792--com.nimbusds__content-type__2.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.12.0.jar:/databricks/jars/proto--logs--activity--lakeviewconfig--lakeview_config_service_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--sql--catalyst--catalyst-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded---362429206--io.opencensus__opencensus-exporter-trace-jaeger__0.31.1.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1456696800--io.grpc__grpc-stub__1.32.1.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--254935968--com.google.code.gson__gson__2.8.6.jar:/databricks/jars/common--util--crypt-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.scalacheck--scalacheck_2.12--org.scalacheck__scalacheck_2.12__1.14.2.jar:/databricks/jars/----ws_3_3--vendor--connect--server--spark_connect_proto_lib_shaded---1222865220--com.google.android__annotations__4.1.1.4.jar:/databricks/jars/common--conf--dbconf--dbconf-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/proto--logs--activity--webapp--webapp_service_proto-spark_3.3_2.12-scalabp.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.3_2.12--1057051737--gcsio_proto_library-speed-src.jar:/databricks/jars/common--logging--service-request--utils-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---502712802--io.grpc__grpc-api__1.45.1.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/spark--logging--logging-spark_3.3_2.12_deploy.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:/databricks/jars/common--conf--parser--parser-spark_3.3_2.12_deploy.jar:/databricks/jars/common--spark--versions-info-service--versions-info-service-spark_3.3_2.12_deploy.jar:/databricks/jars/third_party--azure--azure-storage_shaded--966101332--com.microsoft.azure__azure-storage__8.6.4.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----ws_3_3--mvn--hadoop3--org.lz4--lz4-java--org.lz4__lz4-java__1.8.0.jar\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nrage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-08 21:57:08,248] INFO RemoteLogManagerConfig values: \n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-08 21:57:08,302] INFO KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.gzip.level = -1\n\tcompression.lz4.level = 9\n\tcompression.type = producer\n\tcompression.zstd.level = 3\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.bootstrap.servers = []\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\teligible.leader.replicas.enable = false\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.migration.policy = disabled\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.append.linger.ms = 10\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.rebalance.protocols = [classic]\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.8-IV0\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT\n\tlisteners = PLAINTEXT://:9092\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dir.failure.timeout.ms = 30000\n\tlog.dirs = /tmp/kafka-logs\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.initial.task.delay.ms = 30000\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmax.request.partition.size.limit = 2000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.allow.dn.changes = false\n\tssl.allow.san.changes = false\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttelemetry.max.bytes = 1048576\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 1\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tunstable.feature.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = localhost:2181\n\tzookeeper.connection.timeout.ms = 18000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.metadata.migration.min.batch.size = 200\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2024-10-08 21:57:08,311] INFO RemoteLogManagerConfig values: \n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-08 21:57:08,652] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-08 21:57:08,663] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-08 21:57:08,663] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-08 21:57:08,708] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-08 21:57:08,786] INFO [KafkaServer id=0] Rewriting /tmp/kafka-logs/meta.properties (kafka.server.KafkaServer)\n[2024-10-08 21:57:09,029] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n[2024-10-08 21:57:09,052] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2024-10-08 21:57:09,091] INFO Loaded 0 logs in 61ms (kafka.log.LogManager)\n[2024-10-08 21:57:09,108] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2024-10-08 21:57:09,112] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2024-10-08 21:57:09,313] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2024-10-08 21:57:09,399] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n[2024-10-08 21:57:09,429] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-08 21:57:09,528] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2024-10-08 21:57:11,125] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2024-10-08 21:57:11,207] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2024-10-08 21:57:11,234] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2024-10-08 21:57:11,403] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,410] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,412] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,415] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,421] INFO [ExpirationReaper-0-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,474] INFO [AddPartitionsToTxnSenderThread-0]: Starting (kafka.server.AddPartitionsToTxnManager)\n[2024-10-08 21:57:11,483] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2024-10-08 21:57:11,524] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n[2024-10-08 21:57:11,619] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1728424631597,1728424631597,1,0,0,72058868397572096,254,0,25\n (kafka.zk.KafkaZkClient)\n[2024-10-08 21:57:11,621] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://1008-204032-lfazxauf-10-172-210-210:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n[2024-10-08 21:57:11,836] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,900] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,914] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:11,932] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n[2024-10-08 21:57:12,014] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-08 21:57:12,027] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-08 21:57:12,045] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-08 21:57:12,141] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-08 21:57:12,149] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-08 21:57:12,158] INFO [TxnMarkerSenderThread-0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2024-10-08 21:57:12,176] INFO [MetadataCache brokerId=0] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)\n[2024-10-08 21:57:12,510] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-08 21:57:12,615] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,644] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1008-204032-lfazxauf-10-172-210-210/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,694] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,779] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n[2024-10-08 21:57:12,796] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n[2024-10-08 21:57:12,797] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,797] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1008-204032-lfazxauf-10-172-210-210/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,798] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-08 21:57:12,811] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2024-10-08 21:57:12,863] INFO [KafkaServer id=0] Start processing authorizer futures (kafka.server.KafkaServer)\n[2024-10-08 21:57:12,866] INFO [KafkaServer id=0] End processing authorizer futures (kafka.server.KafkaServer)\n[2024-10-08 21:57:12,867] INFO [KafkaServer id=0] Start processing enable request processing future (kafka.server.KafkaServer)\n[2024-10-08 21:57:12,868] INFO [KafkaServer id=0] End processing enable request processing future (kafka.server.KafkaServer)\n[2024-10-08 21:57:12,876] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-08 21:57:12,876] INFO Kafka commitId: 30969fa33c185e88 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-08 21:57:12,877] INFO Kafka startTimeMs: 1728424632869 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-08 21:57:12,887] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n[2024-10-08 21:57:13,214] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node 1008-204032-lfazxauf-10-172-210-210:9092 (id: 0 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2024-10-08 21:57:13,264] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node 1008-204032-lfazxauf-10-172-210-210:9092 (id: 0 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2024-10-08 21:58:49,431] INFO Creating topic test with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n[2024-10-08 21:58:49,658] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-0) (kafka.server.ReplicaFetcherManager)\n[2024-10-08 21:58:49,808] INFO [LogLoader partition=test-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2024-10-08 21:58:49,850] INFO Created log for partition test-0 in /tmp/kafka-logs/test-0 with properties {} (kafka.log.LogManager)\n[2024-10-08 21:58:49,855] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition)\n[2024-10-08 21:58:49,860] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2024-10-08 22:09:07,792] INFO [NodeToControllerChannelManager id=0 name=forwarding] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n"
     ]
    }
   ],
   "source": [
    "%sh ls -ltr\n",
    "cd kafka_2.12-3.8.0\n",
    "bin/kafka-server-start.sh config/server.properties"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 480967230790649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4e_kafka_step2_startKafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
